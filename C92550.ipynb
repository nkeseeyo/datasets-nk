{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nkeseeyo/datasets-nk/blob/main/C92550.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyTMix2Dx2PY"
      },
      "source": [
        "## Environment & Folder Setup\n",
        "\n",
        "This cell creates three folders—data, results, and models—to organise the project files. It ensures clean storage for datasets, output results, and trained models, preventing file-path errors during execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kYWZLtVYxz1p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "os.makedirs(\"models\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ea7128f",
        "outputId": "0e40f01e-2cbc-423e-e424-8d7b143f6377"
      },
      "source": [
        "# Load the BUSI dataset\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "train_ultra = datagen.flow_from_directory(\n",
        "    \"/content/drive/MyDrive/datasets/DATASETSWORK/Code/dataset/Breast Ultrasound/Dataset_BUSI_with_GT\",\n",
        "    target_size=(128,128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_ultra = datagen.flow_from_directory(\n",
        "    \"/content/drive/MyDrive/datasets/DATASETSWORK/Code/dataset/Breast Ultrasound/Dataset_BUSI_with_GT\",\n",
        "    target_size=(128,128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Display the first batch of training data\n",
        "images, labels = next(train_ultra)\n",
        "print(\"Shape of the first batch of images:\", images.shape)\n",
        "print(\"Shape of the first batch of labels:\", labels.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1263 images belonging to 3 classes.\n",
            "Found 315 images belonging to 3 classes.\n",
            "Shape of the first batch of images: (32, 128, 128, 3)\n",
            "Shape of the first batch of labels: (32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92fd9678",
        "outputId": "341fb89e-63a9-4c34-8305-0cb484fc3859"
      },
      "source": [
        "!ls \"/content/drive/MyDrive/datasets/DATASETSWORK/\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/drive/MyDrive/datasets/DATASETSWORK/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/datasets/DATASETSWORK/code\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5O4fqQhoGws",
        "outputId": "96e11cec-1c11-436b-a1cd-7d4638f8c1b0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/drive/MyDrive/datasets/DATASETSWORK/code': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmZlByODodpq",
        "outputId": "f0532e68-f2d3-4242-e702-133da63d0ebc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/datasets\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9MjCL9EopF7",
        "outputId": "d212c4b4-08cd-4134-fff4-f02328ee847f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATASETSWORK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/datasets/DATASETSWORK\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AWtE551ovxC",
        "outputId": "9bf91d8e-85a1-4325-f5ed-87ebdc57bb69"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/datasets/DATASETSWORK/Code\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Jf-KymWo1WJ",
        "outputId": "ebade6a2-1829-4177-83c1-f2cf6e424723"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C92550.ipynb  data  dataset  models  requirements.txt  results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/datasets/DATASETSWORK/Code/dataset\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81e7s9y7p2IY",
        "outputId": "93093f5e-a75f-4b61-f963-1f67de3efa3e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Breast Cancer YasserH'  'MIAS Mammography Dataset'\n",
            "'Breast Ultrasound'\t 'Wisconsin Diagnostic'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/datasets/DATASETSWORK/Code/dataset/Breast Ultrasound\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBU0Yfizp-TJ",
        "outputId": "f2a19d18-3ace-4efa-ccd3-5f57b0bf3c37"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset_BUSI_with_GT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/datasets/DATASETSWORK/Code/dataset/Breast Ultrasound/Dataset_BUSI_with_GT\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmWCv2l-qEdM",
        "outputId": "85a425c7-fbc4-4aa4-8421-806f81cf7b59"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "benign\tmalignant  normal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1bHIo3E3PU6"
      },
      "source": [
        "## Install Dependencies\n",
        "\n",
        "This cell displays the active Python interpreter path and installs all essential libraries—joblib, pandas, numpy, scikit-learn, imbalanced-learn, shap, matplotlib, seaborn, kaggle, and tensorflow—ensuring every dependency required for data processing and model training is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-qw9fLqXXq5",
        "outputId": "87b46e5a-4337-487c-9d17-01dbae1ff928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/python3\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.0)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (0.49.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.12/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.12/dist-packages (from shap) (25.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from shap) (4.15.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.10.5)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.executable)\n",
        "!{sys.executable} -m pip install joblib pandas numpy scikit-learn imbalanced-learn shap matplotlib seaborn kaggle tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YARpB-VyFqv"
      },
      "source": [
        "## Imports & Helper Functions\n",
        "\n",
        "This cell imports essential libraries, defines preprocessing, evaluation, and explainability functions, and sets a random seed. It prepares reusable utilities for model training, data cleaning, performance measurement, and SHAP-based feature importance visualisation in breast cancer classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BUOc6MJ_yGcX"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from typing import Tuple, Dict, Any\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_predict\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
        "    confusion_matrix, roc_curve, auc, classification_report\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    shap_installed = True\n",
        "except:\n",
        "    shap_installed = False\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "def ensure_binary(y: pd.Series) -> pd.Series:\n",
        "    \"\"\"Map common breast-cancer labels to 0/1 (0=benign, 1=malignant).\"\"\"\n",
        "    y2 = y.copy()\n",
        "    # Common label names\n",
        "    mapping_candidates = [\n",
        "        {'M':1, 'B':0, 'benign':0, 'malignant':1, 'Benign':0, 'Malignant':1},\n",
        "        {'1':1, '0':0}, {1:1, 0:0}, {'yes':1, 'no':0}, {'Y':1, 'N':0}\n",
        "    ]\n",
        "    if y2.dtype == 'O':\n",
        "        y2 = y2.str.strip()\n",
        "    for mp in mapping_candidates:\n",
        "        try:\n",
        "            return y2.map(mp).astype(int)\n",
        "        except Exception:\n",
        "            pass\n",
        "    if pd.api.types.is_numeric_dtype(y2):\n",
        "        return (y2.astype(float) > 0).astype(int)\n",
        "    classes = {cls:i for i, cls in enumerate(sorted(y2.unique()))}\n",
        "    return y2.map(classes).astype(int)\n",
        "\n",
        "def basic_numeric_categoricals(X: pd.DataFrame) -> Tuple[list, list]:\n",
        "    \"\"\"Split columns into numeric/categorical for preprocessing.\"\"\"\n",
        "    num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "    cat_cols = [c for c in X.columns if c not in num_cols]\n",
        "    return num_cols, cat_cols\n",
        "\n",
        "def build_preprocessor(X: pd.DataFrame, scale: bool=True) -> ColumnTransformer:\n",
        "    num_cols, cat_cols = basic_numeric_categoricals(X)\n",
        "    num_tf = []\n",
        "    if len(num_cols):\n",
        "        num_tf = [('num_impute', SimpleImputer(strategy='median'), num_cols)]\n",
        "        if scale:\n",
        "            num_tf.append(('num_scale', StandardScaler(), num_cols))\n",
        "    cat_tf = []\n",
        "    if len(cat_cols):\n",
        "        cat_tf = [('cat_impute', SimpleImputer(strategy='most_frequent'), cat_cols)]\n",
        "\n",
        "    transformers = []\n",
        "    transformers.extend([t for t in num_tf])\n",
        "    transformers.extend([t for t in cat_tf])\n",
        "    if not transformers:\n",
        "        raise ValueError(\"No columns found to preprocess.\")\n",
        "    return ColumnTransformer(transformers=transformers, remainder='drop')\n",
        "\n",
        "def evaluate_and_plot(y_true, y_prob, y_pred, model_name, dataset_name, out_dir=\"results\"):\n",
        "    \"\"\"Compute metrics, save CM & ROC plots, return metrics dict.\"\"\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    metrics = {\n",
        "        \"dataset\": dataset_name,\n",
        "        \"model\": model_name,\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
        "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
        "    }\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(y_true, y_prob)\n",
        "    except:\n",
        "        roc_auc = np.nan\n",
        "    metrics[\"roc_auc\"] = roc_auc\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    fig_cm, ax = plt.subplots()\n",
        "    sns.heatmap(cm, annot=True, fmt='d', ax=ax)\n",
        "    ax.set_title(f\"Confusion Matrix: {model_name} on {dataset_name}\")\n",
        "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
        "    cm_path = f\"{out_dir}/CM_{dataset_name}_{model_name}.png\".replace(\" \", \"_\")\n",
        "    fig_cm.savefig(cm_path, bbox_inches=\"tight\", dpi=150)\n",
        "    plt.close(fig_cm)\n",
        "\n",
        "    # ROC curve\n",
        "    if not np.isnan(roc_auc):\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "        fig_roc, ax = plt.subplots()\n",
        "        ax.plot(fpr, tpr, label=f\"AUC={auc(fpr,tpr):.3f}\")\n",
        "        ax.plot([0,1], [0,1], linestyle='--')\n",
        "        ax.set_title(f\"ROC: {model_name} on {dataset_name}\")\n",
        "        ax.set_xlabel(\"False Positive Rate\"); ax.set_ylabel(\"True Positive Rate\")\n",
        "        ax.legend()\n",
        "        roc_path = f\"{out_dir}/ROC_{dataset_name}_{model_name}.png\".replace(\" \", \"_\")\n",
        "        fig_roc.savefig(roc_path, bbox_inches=\"tight\", dpi=150)\n",
        "        plt.close(fig_roc)\n",
        "\n",
        "    # Classification report text\n",
        "    report_txt = classification_report(y_true, y_pred, digits=3)\n",
        "    with open(f\"{out_dir}/REPORT_{dataset_name}_{model_name}.txt\".replace(\" \", \"_\"), \"w\") as f:\n",
        "        f.write(report_txt)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def maybe_shap_summary(fitted_pipeline, X_tr, dataset_name, model_name, out_dir=\"results\"):\n",
        "    \"\"\"Optional SHAP summary for RF/LR if shap is installed.\"\"\"\n",
        "    if not shap_installed:\n",
        "        return\n",
        "    # Extract final estimator\n",
        "    try:\n",
        "        final_est = fitted_pipeline.named_steps['clf']\n",
        "    except:\n",
        "        # imblearn Pipeline naming\n",
        "        final_est = fitted_pipeline.named_steps.get('clf', None)\n",
        "    if final_est is None:\n",
        "        return\n",
        "    if isinstance(final_est, (RandomForestClassifier, LogisticRegression)):\n",
        "        # Get processed X for SHAP\n",
        "        try:\n",
        "            preproc = fitted_pipeline.named_steps['preproc']\n",
        "            X_proc = preproc.transform(X_tr)\n",
        "        except:\n",
        "            X_proc = X_tr.values\n",
        "        explainer = shap.Explainer(final_est, X_proc)\n",
        "        shap_values = explainer(X_proc[:100])  # subsample for speed\n",
        "        plt.figure()\n",
        "        shap.plots.beeswarm(shap_values, show=False, max_display=15)\n",
        "        p = f\"{out_dir}/SHAP_{dataset_name}_{model_name}.png\".replace(\" \", \"_\")\n",
        "        plt.title(f\"SHAP Summary: {model_name} on {dataset_name}\")\n",
        "        plt.savefig(p, bbox_inches='tight', dpi=150)\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf-_nJPoy4EZ"
      },
      "source": [
        "## Dataset Loaders\n",
        "This cell defines dataset loader functions for various breast cancer datasets. It handles tabular data (Kaggle and Wisconsin) and image data (BUSI and MIAS) using preprocessing, directory-based loading, and Keras ImageDataGenerator for model-ready input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "87sP8pE2y4uI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer as sk_breast\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Utility\n",
        "def ensure_binary(y: pd.Series):\n",
        "    \"\"\"Convert string labels like 'Benign'/'Malignant' into 0/1 numeric.\"\"\"\n",
        "    mapping = {'B':0, 'M':1, 'benign':0, 'malignant':1, 'normal':0, 'Benign':0, 'Malignant':1, 'Normal':0}\n",
        "    if y.dtype == 'O':\n",
        "        return y.map(mapping).astype(int)\n",
        "    return y.astype(int)\n",
        "\n",
        "\n",
        "# 1. Kaggle Breast Cancer YasserH\n",
        "def load_dataset_yasserh(path=\"dataset/Breast Cancer YasserH/breast-cancer.csv\"):\n",
        "    df = pd.read_csv(path)\n",
        "    target_col = None\n",
        "    for col in ['diagnosis','Class','target','Outcome']:\n",
        "        if col in df.columns:\n",
        "            target_col = col\n",
        "            break\n",
        "    if target_col is None:\n",
        "        target_col = df.columns[-1]\n",
        "    y = ensure_binary(df[target_col])\n",
        "    X = df.drop(columns=[target_col])\n",
        "    for c in ['id','ID','Unnamed: 32']:\n",
        "        if c in X.columns:\n",
        "            X = X.drop(columns=[c])\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# 2. Wisconsin Diagnostic Dataset\n",
        "def load_dataset_wisconsin(path=\"dataset/Wisconsin Diagnostic/data.csv\"):\n",
        "    df = pd.read_csv(path)\n",
        "    # Usually columns: ID, diagnosis, 30 features\n",
        "    if 'diagnosis' in df.columns:\n",
        "        y = ensure_binary(df['diagnosis'])\n",
        "        X = df.drop(columns=['diagnosis'])\n",
        "    else:\n",
        "        y = ensure_binary(df.iloc[:, 1])\n",
        "        X = df.drop(df.columns[[0, 1]], axis=1)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# 3. Breast Ultrasound Dataset (BUSI)\n",
        "def load_dataset_ultrasound(path=\"dataset/Breast Ultrasound/Dataset_BUSI_with_GT\", img_size=(128,128), batch_size=32):\n",
        "    \"\"\"\n",
        "    Loads BUSI ultrasound images from three folders: benign, malignant, normal.\n",
        "    Returns ImageDataGenerator flow for training/testing.\n",
        "    \"\"\"\n",
        "    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "    train_gen = datagen.flow_from_directory(\n",
        "        path,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        subset='training'\n",
        "    )\n",
        "    val_gen = datagen.flow_from_directory(\n",
        "        path,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        subset='validation'\n",
        "    )\n",
        "    return train_gen, val_gen\n",
        "\n",
        "\n",
        "# 4. MIAS Mammography Dataset\n",
        "def load_dataset_mias(path=\"dataset/MIAS Mammography Dataset/all-mias\", img_size=(128,128), batch_size=32):\n",
        "    \"\"\"\n",
        "    Loads MIAS images (converted to PNG/JPG/PGM) using ImageDataGenerator.\n",
        "    Expects folders containing labeled subdirectories or filenames with labels.\n",
        "    If only one folder exists, you can create subfolders manually by label.\n",
        "    \"\"\"\n",
        "    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "    # If the dataset has subfolders (e.g., benign/malignant)\n",
        "    if any(os.path.isdir(os.path.join(path, d)) for d in os.listdir(path)):\n",
        "        train_gen = datagen.flow_from_directory(\n",
        "            path,\n",
        "            target_size=img_size,\n",
        "            batch_size=batch_size,\n",
        "            class_mode='binary',\n",
        "            subset='training'\n",
        "        )\n",
        "        val_gen = datagen.flow_from_directory(\n",
        "            path,\n",
        "            target_size=img_size,\n",
        "            batch_size=batch_size,\n",
        "            class_mode='binary',\n",
        "            subset='validation'\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"MIAS dataset needs subfolders (benign/malignant) or labeled filenames.\")\n",
        "    return train_gen, val_gen\n",
        "\n",
        "\n",
        "# 5. Sklearn Built-In Dataset (optional for comparison)\n",
        "def load_dataset_sklearn():\n",
        "    data = sk_breast()\n",
        "    X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "    y = pd.Series(data.target)\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSYtHWAmy8F_"
      },
      "source": [
        "## Define the 4 Algorithms\n",
        "This cell defines a function that returns four machine learning models—Random Forest, SVM, KNN, and Logistic Regression—each configured with balanced class weights and tuned parameters for reliable breast cancer classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KS10V7-Fy87f"
      },
      "outputs": [],
      "source": [
        "def get_models() -> Dict[str, Any]:\n",
        "    models = {\n",
        "        \"RandomForest\": RandomForestClassifier(\n",
        "            n_estimators=300, random_state=RANDOM_STATE, class_weight='balanced'\n",
        "        ),\n",
        "        \"SVM\": SVC(\n",
        "            kernel='rbf', probability=True, random_state=RANDOM_STATE, class_weight='balanced'\n",
        "        ),\n",
        "        \"KNN\": KNeighborsClassifier(n_neighbors=7),\n",
        "        \"LogisticRegression\": LogisticRegression(\n",
        "            max_iter=2000, random_state=RANDOM_STATE, class_weight='balanced'\n",
        "        )\n",
        "    }\n",
        "    return models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYvLqqXkzAFw"
      },
      "source": [
        "## Unified Train/Eval Loop (Produces 16 Results)\n",
        "This cell defines a function that trains all machine learning models on one dataset. It splits data, applies preprocessing, handles imbalance with SMOTE, evaluates performance using multiple metrics, saves visual results, and exports trained models and CSV summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ISpW5VLIzBZ3"
      },
      "outputs": [],
      "source": [
        "def train_one_dataset(X, y, dataset_name: str, models: Dict[str, Any], out_dir=\"results\") -> pd.DataFrame:\n",
        "    results = []\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # Split once for fair comparison\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "        X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    # Preprocessor (scale beneficial for SVM/KNN/LR; ok for RF too)\n",
        "    preproc = build_preprocessor(X_tr, scale=True)\n",
        "\n",
        "    for model_name, clf in models.items():\n",
        "        # Imbalanced pipeline: Preprocess -> SMOTE -> Classifier\n",
        "        pipe = ImbPipeline(steps=[\n",
        "            ('preproc', preproc),\n",
        "            ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
        "            ('clf', clf)\n",
        "        ])\n",
        "\n",
        "        # Fit\n",
        "        pipe.fit(X_tr, y_tr)\n",
        "\n",
        "        # Predict proba and labels\n",
        "        try:\n",
        "            y_prob = pipe.predict_proba(X_te)[:,1]\n",
        "        except:\n",
        "            # For models without predict_proba\n",
        "            y_prob = pipe.decision_function(X_te)\n",
        "            # scale to 0-1\n",
        "            y_prob = (y_prob - y_prob.min()) / (y_prob.max() - y_prob.min() + 1e-8)\n",
        "        y_pred = pipe.predict(X_te)\n",
        "\n",
        "        # Evaluate + plots\n",
        "        metrics = evaluate_and_plot(y_te, y_prob, y_pred, model_name, dataset_name, out_dir=out_dir)\n",
        "        results.append(metrics)\n",
        "\n",
        "        maybe_shap_summary(pipe, X_tr, dataset_name, model_name, out_dir=out_dir)\n",
        "\n",
        "        # Save model\n",
        "        joblib.dump(pipe, f\"models/{dataset_name}_{model_name}.joblib\".replace(\" \", \"_\"))\n",
        "\n",
        "    df_res = pd.DataFrame(results)\n",
        "    csv_path = f\"{out_dir}/ALL_METRICS.csv\"\n",
        "    if os.path.exists(csv_path):\n",
        "        existing = pd.read_csv(csv_path)\n",
        "        pd.concat([existing, df_res], ignore_index=True).to_csv(csv_path, index=False)\n",
        "    else:\n",
        "        df_res.to_csv(csv_path, index=False)\n",
        "\n",
        "    df_res.to_csv(f\"{out_dir}/METRICS_{dataset_name}.csv\".replace(\" \", \"_\"), index=False)\n",
        "    return df_res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhC7u5NtzDuw"
      },
      "source": [
        "## Run All 4 Datasets × 4 Models\n",
        "This cell sequentially trains models on all datasets. It runs tabular datasets through machine learning pipelines, loads image datasets for CNN preparation, merges tabular results into one summary file, and confirms readiness for deep learning training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nz_bXynLzFfA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "03220ecd-2ec6-491d-a821-77f9fc445bad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle_YasserH dataset skipped: The beeswarm plot does not support plotting explanations with instances that have more than one dimension!\n",
            "Wisconsin_Diagnostic dataset skipped: Input X contains NaN.\n",
            "SMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
            "Found 1263 images belonging to 3 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Unnamed: 32']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
            "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
            "  T = new_sum / new_sample_count\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
            "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 315 images belonging to 3 classes.\n",
            "Ultrasound dataset loaded successfully — ready for CNN training.\n",
            "MIAS dataset skipped: MIAS dataset needs subfolders (benign/malignant) or labeled filenames.\n",
            "No tabular datasets ran successfully. Please check file paths.\n",
            "\n",
            " Tabular datasets processed. Image datasets (Ultrasound, MIAS) ready for CNN model training.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "all_results = []\n",
        "\n",
        "models = get_models()\n",
        "\n",
        "# Kaggle Breast Cancer (YasserH - tabular)\n",
        "try:\n",
        "    X1, y1 = load_dataset_yasserh(\"/content/drive/MyDrive/datasets/DATASETSWORK/Code/dataset/Breast Cancer YasserH/breast-cancer.csv\")\n",
        "    res1 = train_one_dataset(X1, y1, \"Kaggle_YasserH\", models)\n",
        "    all_results.append(res1)\n",
        "except Exception as e:\n",
        "    print(\"Kaggle_YasserH dataset skipped:\", e)\n",
        "\n",
        "\n",
        "# Wisconsin Diagnostic Dataset (tabular)\n",
        "try:\n",
        "    X2, y2 = load_dataset_wisconsin(\"/content/drive/MyDrive/datasets/DATASETSWORK/Code/dataset/Wisconsin Diagnostic/data.csv\")\n",
        "    res2 = train_one_dataset(X2, y2, \"Wisconsin_Diagnostic\", models)\n",
        "    all_results.append(res2)\n",
        "except Exception as e:\n",
        "    print(\"Wisconsin_Diagnostic dataset skipped:\", e)\n",
        "\n",
        "\n",
        "# Breast Ultrasound Dataset (BUSI - image dataset)\n",
        "try:\n",
        "    train_ultra, val_ultra = load_dataset_ultrasound(\"/content/drive/MyDrive/datasets/DATASETSWORK/Code/dataset/Breast Ultrasound/Dataset_BUSI_with_GT\")\n",
        "    print(\"Ultrasound dataset loaded successfully — ready for CNN training.\")\n",
        "except Exception as e:\n",
        "    print(\"Breast Ultrasound dataset skipped:\", e)\n",
        "\n",
        "\n",
        "# MIAS Mammography Dataset (image dataset)\n",
        "try:\n",
        "    train_mias, val_mias = load_dataset_mias(\"/content/drive/MyDrive/datasets/DATASETSWORK/Code/dataset/MIAS Mammography Dataset/all-mias\")\n",
        "    print(\"MIAS dataset loaded successfully — ready for CNN training.\")\n",
        "except Exception as e:\n",
        "    print(\"MIAS dataset skipped:\", e)\n",
        "\n",
        "\n",
        "# Combine tabular model results (first two datasets)\n",
        "if len(all_results):\n",
        "    final_table = pd.concat(all_results, ignore_index=True)\n",
        "    final_table.sort_values([\"dataset\", \"roc_auc\"], ascending=[True, False], inplace=True)\n",
        "    display(final_table)\n",
        "    final_table.to_csv(\"results/SUMMARY_TABULAR_RESULTS.csv\", index=False)\n",
        "    print(\"Tabular model results saved to results/SUMMARY_TABULAR_RESULTS.csv\")\n",
        "else:\n",
        "    print(\"No tabular datasets ran successfully. Please check file paths.\")\n",
        "\n",
        "print(\"\\n Tabular datasets processed. Image datasets (Ultrasound, MIAS) ready for CNN model training.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "import pandas as pd\n",
        "\n",
        "# Load Wisconsin dataset\n",
        "wisconsin_path = \"/content/drive/MyDrive/datasets/DATASETSWORK/Code/dataset/Wisconsin Diagnostic/data.csv\"  # adjust if needed\n",
        "df_wisconsin = pd.read_csv(wisconsin_path)\n",
        "\n",
        "# Drop irrelevant column\n",
        "df_wisconsin = df_wisconsin.drop(columns=[\"Unnamed: 32\"], errors='ignore')\n",
        "\n",
        "# Separate features and labels\n",
        "X = df_wisconsin.drop(columns=['diagnosis'])\n",
        "y = df_wisconsin['diagnosis']\n",
        "\n",
        "# Replace NaN values\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X = imputer.fit_transform(X)\n",
        "\n",
        "print(\"✅ Wisconsin dataset cleaned successfully — no NaN values remaining.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fpoDiIl6mGI",
        "outputId": "d6d74e93-766f-4939-becf-493cfc7ae612"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Wisconsin dataset cleaned successfully — no NaN values remaining.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/datasets/DATASETSWORK/Code/dataset\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCBk7pnW5wkU",
        "outputId": "092edd41-9f7e-4932-e4cd-06c357d91b26"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Breast Cancer YasserH'  'MIAS Mammography Dataset'\n",
            "'Breast Ultrasound'\t 'Wisconsin Diagnostic'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/datasets/DATASETSWORK/Code/dataset/Breast Cancer YasserH\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR-rsuWX6Cl4",
        "outputId": "9691a406-201c-4ce2-92a8-9dffd44a8e79"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "breast-cancer.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1vQ2RZeXXq-"
      },
      "source": [
        "## CNN Model\n",
        "This cell builds and trains a Convolutional Neural Network (CNN) for breast ultrasound images. It uses convolution and pooling layers for feature extraction, compiles the model, trains it for ten epochs, and saves the trained CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLSU3y22XXq-",
        "outputId": "30b01737-29af-4a59-c50a-8c22061233d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Found 1263 images belonging to 3 classes.\n",
            "Found 315 images belonging to 3 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m720s\u001b[0m 18s/step - accuracy: 0.4104 - loss: 0.8884 - val_accuracy: 0.3651 - val_loss: 0.6704\n",
            "Epoch 2/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 1s/step - accuracy: 0.4829 - loss: 0.5258 - val_accuracy: 0.3524 - val_loss: 0.5396\n",
            "Epoch 3/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 1s/step - accuracy: 0.5583 - loss: -0.1224 - val_accuracy: 0.3873 - val_loss: 0.3697\n",
            "Epoch 4/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.5187 - loss: -2.0184 - val_accuracy: 0.3524 - val_loss: -1.0487\n",
            "Epoch 5/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 1s/step - accuracy: 0.5577 - loss: -14.6068 - val_accuracy: 0.3524 - val_loss: -15.8573\n",
            "Epoch 6/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 1s/step - accuracy: 0.5732 - loss: -86.2320 - val_accuracy: 0.4508 - val_loss: -93.6498\n",
            "Epoch 7/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 1s/step - accuracy: 0.5159 - loss: -421.5149 - val_accuracy: 0.3460 - val_loss: -623.5420\n",
            "Epoch 8/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 1s/step - accuracy: 0.5070 - loss: -1448.9091 - val_accuracy: 0.3206 - val_loss: -2595.3511\n",
            "Epoch 9/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 1s/step - accuracy: 0.4552 - loss: -6424.9312 - val_accuracy: 0.3905 - val_loss: -12563.7080\n",
            "Epoch 10/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 1s/step - accuracy: 0.5004 - loss: -24911.3164 - val_accuracy: 0.3492 - val_loss: -35555.6484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# STEP 1 — Install required packages\n",
        "!pip install tensorflow\n",
        "\n",
        "# STEP 2 — Import libraries\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "import os\n",
        "\n",
        "# STEP 3 — Load the BUSI dataset\n",
        "# (Make sure you have this folder structure in your Colab files:)\n",
        "# dataset/Breast Ultrasound/Dataset_BUSI_with_GT/benign\n",
        "# dataset/Breast Ultrasound/Dataset_BUSI_with_GT/malignant\n",
        "# dataset/Breast Ultrasound/Dataset_BUSI_with_GT/normal\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "train_ultra = datagen.flow_from_directory(\n",
        "    \"/content/drive/MyDrive/datasets/DATASETSWORK/Code/dataset/Breast Ultrasound/Dataset_BUSI_with_GT\",\n",
        "    target_size=(128,128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_ultra = datagen.flow_from_directory(\n",
        "    \"/content/drive/MyDrive/datasets/DATASETSWORK/Code/dataset/Breast Ultrasound/Dataset_BUSI_with_GT\",\n",
        "    target_size=(128,128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# STEP 4 — Build the CNN model\n",
        "def build_cnn_model(input_shape=(128,128,3)):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling2D(2,2),\n",
        "        layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        layers.MaxPooling2D(2,2),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# STEP 5 — Train the model\n",
        "cnn = build_cnn_model()\n",
        "history = cnn.fit(train_ultra, validation_data=val_ultra, epochs=10)\n",
        "\n",
        "# STEP 6 — Save the model\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "cnn.save(\"models/CNN_BUSI.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETSpayMlXXq-"
      },
      "source": [
        "## TRAINING\n",
        "This cell performs end-to-end model training for all datasets. It builds and trains ML models for tabular data (Kaggle, Wisconsin) and CNNs for image datasets (BUSI, MIAS), handles preprocessing, evaluates results, saves models, and generates summary files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "lWm1PzN2XXq-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00303e7d-5ee1-4634-86b9-42c2c71aee7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle_YasserH dataset skipped: [Errno 2] No such file or directory: 'dataset/Breast Cancer YasserH/breast-cancer.csv'\n",
            "Wisconsin_Diagnostic dataset skipped: [Errno 2] No such file or directory: 'dataset/Wisconsin Diagnostic/data.csv'\n",
            "BUSI dataset skipped: [Errno 2] No such file or directory: 'dataset/Breast Ultrasound/Dataset_BUSI_with_GT'\n",
            "MIAS dataset skipped: [Errno 2] No such file or directory: 'dataset/MIAS Mammography Dataset/all-mias'\n",
            "No tabular datasets ran successfully. Please check data paths.\n",
            "\n",
            " Training complete for all datasets — tabular + CNN models ready.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Helper: CNN Model\n",
        "def build_cnn_model(input_shape=(128,128,3)):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling2D(2,2),\n",
        "        layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        layers.MaxPooling2D(2,2),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "# Tabular Model Training\n",
        "def train_one_dataset(X, y, dataset_name: str, models: Dict[str, Any], out_dir=\"results\") -> pd.DataFrame:\n",
        "    results = []\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE)\n",
        "    preproc = build_preprocessor(X_tr, scale=True)\n",
        "\n",
        "    for model_name, clf in models.items():\n",
        "        pipe = ImbPipeline(steps=[\n",
        "            ('preproc', preproc),\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
        "            ('clf', clf)\n",
        "        ])\n",
        "\n",
        "        pipe.fit(X_tr, y_tr)\n",
        "\n",
        "        try:\n",
        "            y_prob = pipe.predict_proba(X_te)[:, 1]\n",
        "        except:\n",
        "            y_prob = pipe.decision_function(X_te)\n",
        "            y_prob = (y_prob - y_prob.min()) / (y_prob.max() - y_prob.min() + 1e-8)\n",
        "        y_pred = pipe.predict(X_te)\n",
        "\n",
        "        metrics = evaluate_and_plot(y_te, y_prob, y_pred, model_name, dataset_name, out_dir=out_dir)\n",
        "        results.append(metrics)\n",
        "\n",
        "        try:\n",
        "            maybe_shap_summary(pipe, X_tr, dataset_name, model_name, out_dir=out_dir)\n",
        "        except Exception as e:\n",
        "            print(f\"SHAP skipped for {dataset_name}-{model_name}: {e}\")\n",
        "\n",
        "        joblib.dump(pipe, f\"models/{dataset_name}_{model_name}.joblib\".replace(\" \", \"_\"))\n",
        "\n",
        "    df_res = pd.DataFrame(results)\n",
        "    df_res.to_csv(f\"{out_dir}/METRICS_{dataset_name}.csv\".replace(\" \", \"_\"), index=False)\n",
        "    return df_res\n",
        "\n",
        "\n",
        "# Run All Datasets\n",
        "all_results = []\n",
        "models = get_models()\n",
        "\n",
        "# Kaggle (YasserH)\n",
        "try:\n",
        "    X1, y1 = load_dataset_yasserh(\"dataset/Breast Cancer YasserH/breast-cancer.csv\")\n",
        "    print(f\"Training on Kaggle_YasserH dataset with {X1.shape[0]} samples...\")\n",
        "    res1 = train_one_dataset(X1, y1, \"Kaggle_YasserH\", models)\n",
        "    all_results.append(res1)\n",
        "    print(\"Kaggle_YasserH complete.\")\n",
        "except Exception as e:\n",
        "    print(\"Kaggle_YasserH dataset skipped:\", e)\n",
        "\n",
        "\n",
        "# Wisconsin Diagnostic\n",
        "try:\n",
        "    X2, y2 = load_dataset_wisconsin(\"dataset/Wisconsin Diagnostic/data.csv\")\n",
        "    print(f\"Training on Wisconsin_Diagnostic dataset with {X2.shape[0]} samples...\")\n",
        "    res2 = train_one_dataset(X2, y2, \"Wisconsin_Diagnostic\", models)\n",
        "    all_results.append(res2)\n",
        "    print(\"Wisconsin_Diagnostic complete.\")\n",
        "except Exception as e:\n",
        "    print(\"Wisconsin_Diagnostic dataset skipped:\", e)\n",
        "\n",
        "\n",
        "# Breast Ultrasound (BUSI)\n",
        "try:\n",
        "    train_ultra, val_ultra = load_dataset_ultrasound(\"dataset/Breast Ultrasound/Dataset_BUSI_with_GT\")\n",
        "    print(\"Training CNN on BUSI (Ultrasound) dataset...\")\n",
        "    cnn_ultra = build_cnn_model()\n",
        "    hist_ultra = cnn_ultra.fit(train_ultra, validation_data=val_ultra, epochs=10)\n",
        "    cnn_ultra.save(\"models/CNN_BUSI.h5\")\n",
        "    print(\"BUSI CNN training complete and model saved.\")\n",
        "except Exception as e:\n",
        "    print(\"BUSI dataset skipped:\", e)\n",
        "\n",
        "\n",
        "# MIAS Mammography Dataset\n",
        "try:\n",
        "    train_mias, val_mias = load_dataset_mias(\"dataset/MIAS Mammography Dataset/all-mias\")\n",
        "    print(\"Training CNN on MIAS (Mammography) dataset...\")\n",
        "    cnn_mias = build_cnn_model()\n",
        "    hist_mias = cnn_mias.fit(train_mias, validation_data=val_mias, epochs=10)\n",
        "    cnn_mias.save(\"models/CNN_MIAS.h5\")\n",
        "    print(\"MIAS CNN training complete and model saved.\")\n",
        "except Exception as e:\n",
        "    print(\"MIAS dataset skipped:\", e)\n",
        "\n",
        "\n",
        "# Combine Tabular Results\n",
        "if len(all_results):\n",
        "    final_table = pd.concat(all_results, ignore_index=True)\n",
        "    final_table.sort_values([\"dataset\", \"roc_auc\"], ascending=[True, False], inplace=True)\n",
        "    final_table.to_csv(\"results/SUMMARY_TABULAR_RESULTS.csv\", index=False)\n",
        "    print(\"Tabular results saved: results/SUMMARY_TABULAR_RESULTS.csv\")\n",
        "else:\n",
        "    print(\"No tabular datasets ran successfully. Please check data paths.\")\n",
        "\n",
        "print(\"\\n Training complete for all datasets — tabular + CNN models ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnRUMKJ1zILw"
      },
      "source": [
        "## Nice Summary Plots for Padlet\n",
        "This cell visualises model performance across datasets by plotting ROC AUC scores from the summary CSV. It groups results by dataset, styles the chart with distinct colours, adds labels and legends, and saves the figure for performance comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "H1OMD66PzJIJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81e12a0a-09bb-4b9f-94d8-6f8dc61490ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary bar plot skipped due to error: results/SUMMARY_TABULAR_RESULTS.csv not found. Run training cells first.\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    df_path = \"results/SUMMARY_TABULAR_RESULTS.csv\"\n",
        "    if not os.path.exists(df_path):\n",
        "        raise FileNotFoundError(f\"{df_path} not found. Run training cells first.\")\n",
        "\n",
        "    df = pd.read_csv(df_path)\n",
        "\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    colors = plt.cm.Paired.colors\n",
        "\n",
        "    for i, ds in enumerate(df['dataset'].unique()):\n",
        "        sub = df[df['dataset'] == ds]\n",
        "        x_labels = [f\"{ds}-{m}\" for m in sub['model']]\n",
        "        plt.bar(x_labels, sub['roc_auc'], color=colors[i % len(colors)], label=ds)\n",
        "\n",
        "    plt.xticks(rotation=70, ha='right', fontsize=9)\n",
        "    plt.ylabel(\"ROC AUC\", fontsize=11)\n",
        "    plt.title(\"ROC AUC across Tabular Datasets × 4 Models\", fontsize=13, weight='bold')\n",
        "    plt.legend(title=\"Dataset\", fontsize=9)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    output_path = \"results/BAR_ROC_AUC_TABULAR.png\"\n",
        "    plt.savefig(output_path, dpi=150)\n",
        "    plt.close()\n",
        "    print(f\"Summary bar plot saved to {output_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Summary bar plot skipped due to error:\", e)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}